# ------------------------ lab 2 Task1 ------------------------

from mpi4py import MPI 
import numpy as np 
 
def parallel_sum(array): 
    comm = MPI.COMM_WORLD 
    rank = comm.Get_rank() 
    size = comm.Get_size() 
    
    # Calculate the local sum for each process 
    local_sum = np.sum(array[rank::size]) 
 
    # Gather all local sums on the root process 
    total_sum = comm.reduce(local_sum, op=MPI.SUM, root=0) 
 
    if rank == 0: 
        return total_sum 
    else: 
        return None 
 
if __name__ == '__main__': 
    comm = MPI.COMM_WORLD 
    rank = comm.Get_rank() 
 
    if rank == 0: 
        # Define your array of integers on the root process 
        array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=int) 
    else: 
        array = None 
 
    # Broadcast the array to all processes 
    array = comm.bcast(array, root=0) 
 
    # Calculate the sum in parallel 
    result = parallel_sum(array) 
 
    if rank == 0: 
        print("Sum of array elements:", result)


# ------------------------ lab 2 Task2 ------------------------

def parallel_matrix_multiply(matrix_a, matrix_b): 
    comm = MPI.COMM_WORLD 
    rank = comm.Get_rank() 
    size = comm.Get_size() 
 
    rows_a, cols_a = matrix_a.shape 
    rows_b, cols_b = matrix_b.shape 
 
    if cols_a != rows_b: 
        if rank == 0: 
            print("Error: Number of columns in matrix A must be equal to the number of rows in matrix B.") 
        comm.Abort(1) 
 
    # Divide the work among processes 
    block_size = rows_a // size 
    start_row = rank * block_size 
    end_row = start_row + block_size 
 
    # Broadcast matrix B to all processes 
    matrix_b = comm.bcast(matrix_b, root=0) 
 
    # Perform local matrix multiplication 
    local_result = np.dot(matrix_a[start_row:end_row, :], matrix_b) 
 
    # Gather results on the root process 
    if rank == 0: 
        result = np.empty((rows_a, cols_b), dtype=matrix_a.dtype) 
    else: 
        result = None 
 
    comm.Gather(local_result, result, root=0) 
 
    if rank == 0: 
        return result 
    else: 
        return None 
 
if __name__ == '__main__': 
    comm = MPI.COMM_WORLD 
    rank = comm.Get_rank() 
 
    if rank == 0: 
        # Define your matrices A and B on the root process 
        matrix_a = np.array([[1, 2], [3, 4], [5, 6]], dtype=int) 
        matrix_b = np.array([[7, 8], [9, 10]], dtype=int) 
    else: 
        matrix_a = None 
        matrix_b = None 
 
    # Broadcast matrices A and B to all processes 
    matrix_a = comm.bcast(matrix_a, root=0) 
    matrix_b = comm.bcast(matrix_b, root=0) 
 
    # Calculate the matrix product in parallel 
    result = parallel_matrix_multiply(matrix_a, matrix_b) 
 
    if rank == 0: 
        print("Matrix A:") 
        print(matrix_a) 
        print("Matrix B:") 
        print(matrix_b) 
        print("Matrix Product:") 
        print(result) 



#------------------ Lab 3 -----------------------------


#------------------ Task 1 -----------------------------

#pip install mpi4py

from mpi4py import MPI

def parallel_sum(start, end):
    comm = MPI.COMM_WORLD()
    rank = comm.Get_rank()
    size = comm.Get_size()

    # Calculate the local sum for each process
    local_sum = sum(range(start, end+1, size))
    # Gather all local sums to the root process
    total_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)
    if rank == 0:
        print(f"Total Sum: {total_sum}")

if __name__ == "__main__":
    N = 100
    start = rank + 1
    end = N

    parallel_sum(start, end)


#------------------ Task 2 -----------------------------


from mpi4py import MPI

def point_to_point_communication():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    # Ensure there are at least 2 processes to communicate
    if comm.Get_size() < 2 :
        print("This example requires at least 2 processes.")
        return

    if rank == 0:
        data = "Hello, world!"
        comm.send(data, dest=1)
        print(f"Process {rank} sent: {data}")
    elif rank == 1:
        data = comm.recv(source=0)
        print(f"Process {rank} received: {data}")

if __name__ == "__main__":
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    print(f"Process {rank} starting...")

    # Example of point-to-point communication
    point_to_point_communication()

    comm.barrier()  # Ensure all processes have finished

    print(f"Process {rank} finished.")

#------------------ Task 3 -----------------------------

from mpi4py import MPI

def parallel_sum(numbers):
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    print(rank)
    print(size)
    # Calculate the number of elements each process will sum
    num_elements = len(numbers)
    elements_per_process = num_elements // size
    start = rank * elements_per_process
    end = start + elements_per_process
    print(elements_per_process)
    # Calculate the local sum for each process
    local_sum = sum(numbers[start:end])

    # Gather all local sums to the root process (rank 0)
    total_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)

    if rank == 0:
        print(f"Total Sum: {total_sum}")

if __name__ == "__main__":
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Replace with your list of numbers

    if rank == 0:
        print(f"Numbers: {numbers}")

    parallel_sum(numbers)



#------------------ Task 4 -----------------------------
import threading

def thread_function(thread_id):
    print(f"Hello from thread {thread_id}")

if __name__ == "__main__":
    threads = []

    for i in range(5):
        thread = threading.Thread(target=thread_function, args=(i,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

#------------------ Task 5 -----------------------------
import multiprocessing

def worker_function(data):
    result = []
    for item in data:
        result.append(item * 2)
    return result

if __name__ == "__main__":
    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    num_processes = 4

    # Split the data into chunks for each process
    chunk_size = len(data) // num_processes
    data_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]

    pool = multiprocessing.Pool(processes=num_processes)

    # Map the worker function to each data chunk in parallel
    results = pool.map(worker_function, data_chunks)

    # Join the results from all processes
    final_result = []
    for result in results:
        final_result.extend(result)

    print("Final Result:", final_result)


#------------------ Task 6 -----------------------------
from mpi4py import MPI
from multiprocessing import Pool

def square(number):
    return number ** 2

def parallel_square(numbers):
    with Pool(processes=4) as pool:  # You can adjust the number of processes as needed
        result = pool.map(square, numbers)
    return result

def main():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    if rank == 0:
        numbers = list(range(1, 11))  # List of numbers to square
    else:
        numbers = None

    numbers = comm.bcast(numbers, root=0)

    local_numbers = numbers[rank::comm.Get_size()]
    local_result = parallel_square(local_numbers)
    result = comm.gather(local_result, root=0)

    if rank == 0:
        flattened_result = [item for sublist in result for item in sublist]
        print("Squared Numbers:", flattened_result)

if __name__ == '__main__':
    main()


#------------------ Task 7 -----------------------------
#pip install multiprocess

#multiprocess.Process: This class is used to create new processes.
#multiprocess.Lock: A lock is used for synchronizing access to shared data.
#multiprocess.Value: A Value is used to create a shared integer that multiple processes can access safely.
from multiprocess import Process, Lock, Value

def sum_partial_array(arr, start, end, result, lock):
    partial_sum = sum(arr[start:end])
    with lock:
        result.value += partial_sum

def parallel_sum(arr, num_processes=4):
    result = Value('i', 0)
    lock = Lock()
    processes = []

    chunk_size = len(arr) // num_processes
    for i in range(num_processes):
        start = i * chunk_size
        end = start + chunk_size if i < num_processes - 1 else len(arr)
        process = Process(target=sum_partial_array, args=(arr, start, end, result, lock))
        processes.append(process)
        process.start()

    for process in processes:
        process.join()

    return result.value

if __name__ == '__main__':
    arr = list(range(1, 101))  # List of numbers to sum
    num_processes = 4

    result = parallel_sum(arr, num_processes)
    print("Sum:", result)


#------------------ Task 8 -----------------------------
from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Define the matrices A and B
matrix_size = 4
A = np.random.rand(matrix_size, matrix_size)
B = np.random.rand(matrix_size, matrix_size)
result = np.zeros((matrix_size, matrix_size))

# Scatter matrix A and B to all processes
#Divide matrix A into chunks based on the number of processes.
#Scatter these chunks to all processes using the comm.Scatter() method
A_part = np.empty((matrix_size, matrix_size // size), dtype='float64')
B_part = np.empty((matrix_size, matrix_size // size), dtype='float64')
comm.Scatter(A, A_part, root=0)
comm.Scatter(B, B_part, root=0)

# Perform local matrix addition
local_result = A_part + B_part

# Gather local results to the root process
comm.Gather(local_result, result, root=0)

if rank == 0:
    print("Result:")
    print(result)


#------------------ Task 9 -----------------------------
#Parallel Search with Distributed Data
from mpi4py import MPI
import numpy as np

def parallel_search(data, target):
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    # Distribute the data across processes
    chunk_size = len(data) // size
    data_part = data[rank * chunk_size:(rank + 1) * chunk_size]

    # Perform local search
    local_result = [i for i, item in enumerate(data_part) if item == target]

    # Gather local results to the root process
    all_results = comm.gather(local_result, root=0)
#"Indices" refers to the positions in the dataset where the value 5 was found.
    if rank == 0:
        # Merge results from all processes
        result_indices = [idx for sublist in all_results for idx in sublist]
        return result_indices
    else:
        return None

if __name__ == '__main__':
    data = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9] * 10
    target_value = 3

    result = parallel_search(data, target_value)

    if result is not None:
        print(f"Target {target_value} found at indices: {result}")
    else:
        print("Process has no results.")




#------------------ Lab 4 -----------------------------
#------------------ Task1 -----------------------------
        
#include <stdio.h> 
#include <mpi.h> 
int main(int argc, char* argv[]) { 
int rank, size; 
MPI_Init(&argc, &argv); 
MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
MPI_Comm_size(MPI_COMM_WORLD, &size); 
Lab 5 
printf("Hello from process %d of %d\n", rank, size); 
MPI_Finalize(); 
return 0; 
} 


#------------------ Task2 -----------------------------

#include <stdio.h> 
#include <mpi.h> 
int main(int argc, char* argv[]) { 
MPI_Init(&argc, &argv); 
int rank, size; 
MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
MPI_Comm_size(MPI_COMM_WORLD, &size); 
// Define the total number of elements to sum 
const int total_elements = 100; 
// Calculate the number of elements per process 
int elements_per_process = total_elements / size; 
// Allocate memory for local data 
int* local_data = new int[elements_per_process]; 
// Fill local data with some values (for demonstration) 
for (int i = 0; i < elements_per_process; i++) { 
local_data[i] = rank * elements_per_process + i; 
} 
// Compute the local sum 
int local_sum = 0; 
for (int i = 0; i < elements_per_process; i++) { 
local_sum += local_data[i]; 
} 
// Gather all local sums to calculate the global sum 
int global_sum; 
MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD); 
if (rank == 0) { 
printf("Global sum: %d\n", global_sum); 
} 
MPI_Finalize(); 
delete[] local_data; 
return 0; 
} 

#------------------ Task3 -----------------------------

#include <stdio.h> 
#include <mpi.h> 
#include <stdlib.h> 
const int N = 4; // Size of the matrices 
void initializeMatrix(int matrix[N][N], int value) { 
for (int i = 0; i < N; i++) { 
for (int j = 0; j < N; j++) { 
matrix[i][j] = value; 
} 
} 
} 
void printMatrix(int matrix[N][N]) { 
for (int i = 0; i < N; i++) { 
for (int j = 0; j < N; j++) { 
printf("%d\t", matrix[i][j]); 
} 
printf("\n"); 
} 
} 
int main(int argc, char* argv[]) { 
MPI_Init(&argc, &argv); 
int rank, size; 
MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
MPI_Comm_size(MPI_COMM_WORLD, &size); 
int A[N][N], B[N][N], C[N][N]; 
// Initialize matrices A and B 
initializeMatrix(A, 1); 
initializeMatrix(B, 2); 
// Scatter rows of A to different processes 
int rows_per_process = N / size; 
int(*local_A)[N] = (int(*)[N])malloc(rows_per_process * N * sizeof(int)); 
MPI_Scatter(A, rows_per_process * N, MPI_INT, local_A, rows_per_process * N, MPI_INT, 
0, MPI_COMM_WORLD); 
// Broadcast matrix B to all processes 
MPI_Bcast(B, N * N, MPI_INT, 0, MPI_COMM_WORLD); 
// Perform matrix multiplication 
int(*local_C)[N] = (int(*)[N])malloc(rows_per_process * N * sizeof(int)); 
for (int i = 0; i < rows_per_process; i++) { 
for (int j = 0; j < N; j++) { 
local_C[i][j] = 0; 
for (int k = 0; k < N; k++) { 
local_C[i][j] += local_A[i][k] * B[k][j]; 
} 
} 
} 
// Gather results from all processes to root 
MPI_Gather(local_C, rows_per_process * N, MPI_INT, C, rows_per_process * N, MPI_INT, 
0, MPI_COMM_WORLD); 
// Print the result on the root process 
if (rank == 0) { 
printf("Matrix A:\n"); 
printMatrix(A); 
printf("\nMatrix B:\n"); 
printMatrix(B); 
printf("\nMatrix C (Result of A x B):\n"); 
printMatrix(C); 
} 
free(local_A); 
free(local_C); 
MPI_Finalize(); 
return 0; 
} 


#------------------ Lab 06 -----------------------------

#------------------ Task1 -----------------------------


#include <iostream> 
#include <omp.h> 
 
int parallelSum(int arr[], int start, int end) { 
    if (start == end) { 
        return arr[start]; 
    } 
 
    int mid = (start + end) / 2; 
    int left_sum, right_sum; 
 
#pragma omp parallel 
    { 
#pragma omp single 
        { 
#pragma omp task 
            left_sum = parallelSum(arr, start, mid); 
 
#pragma omp task 
            right_sum = parallelSum(arr, mid + 1, end); 
        } 
    } 
 
    return left_sum + right_sum; 
} 
 
int main() { 
    const int arraySize = 1000; 
    int myArray[arraySize]; 
    int sum = 0; 
 
    // Initialize the array with some values 
    for (int i = 0; i < arraySize; i++) { 
        myArray[i] = i; 
 } 
 
    // Initialize OpenMP 
    omp_set_nested(1);  // Enable nested parallelism 
 
#pragma omp parallel 
    { 
#pragma omp single 
        { 
            sum = parallelSum(myArray, 0, arraySize - 1); 
        } 
    } 
 
    std::cout << "The sum of elements in the array is: " << sum << std::endl; 
 
    return 0; 
} 


#------------------ Task2 -----------------------------
#include <iostream> 
#include <omp.h> 
 
int main() { 
    const int arraySize = 180; 
    int myArray[arraySize]; 
 
    // Initialize the array with some values 
    for (int i = 0; i < arraySize; i++) { 
        myArray[i] = i; 
    } 
 
    int sum = 0; 
 
    // Parallelize the task using OpenMP 
#pragma omp parallel for reduction(+:sum) 
    for (int i = 0; i < arraySize; i++) {
        sum += myArray[i]; 
} 
std::cout << "The sum of elements in the array is: " << sum << std::endl; 
return 0; 
}


#------------------ Task1 -----------------------------

#include <iostream>  
#include <omp.h>  
int main() { 
const int N = 1000; 
int num_threads = 4; 
int sum = 0; 
int data[N]; 
// Initialize the data array 
for (int i = 0; i < N; i++) { 
data[i] = i + 1; 
} 
// Create a parallel region using OpenMP 
#pragma omp parallel num_threads(num_threads) reduction(+:sum) 
{ 
int thread_id = omp_get_thread_num(); 
// Calculate the chunk size for load balancing 
int chunk = N / num_threads; 
int start = thread_id * chunk; 
int end = (thread_id == num_threads - 1) ? N : start + chunk; 
// Parallel loop to calculate the sum 
for (int i = start; i < end; i++) { 
sum += data[i]; 
} 
} 
} 
std::cout << "Total sum (OpenMP):" << sum << std::endl; 
return 0; 


#------------------ Task2 -----------------------------
#include <iostream> 
#include <mpi.h> 
 
int main(int argc, char* argv[]) { 
    MPI_Init(&argc, &argv); 
 
    const int N = 1000; 
    int rank, size; 
    int local_sum = 0; 
    int sum = 0; 
 
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
    MPI_Comm_size(MPI_COMM_WORLD, &size); 
 
    // Calculate the chunk size for load balancing 
    int chunk = N / size; 
    int start = rank * chunk; 
    int end = (rank == size - 1) ? N : start + chunk; 
 
    // Calculate the local sum for the assigned data 
    for (int i = start; i < end; i++) { 
        local_sum += i + 1; 
    } 
 
    // Perform an "allreduce" operation to calculate the global sum 
    MPI_Allreduce(&local_sum, &sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD); 
 
    if (rank == 0) { 
        std::cout << "Total sum (MPI): " << sum << std::endl; 
    } 
 
    MPI_Finalize(); 
return 0; 
} 

#------------------ Lab7 -----------------------------
#------------------ Task1 -----------------------------





#------------------ Lab8 -----------------------------
#------------------ Task1 -----------------------------

#include <iostream> 
#include <ctime> 
#include <vector> 
#include <omp.h> 
const int SIZE = 1000000; 
int main() { 
        //initialize vectors 
        std::vector <int> vector1(SIZE, 1); 
        std::vector <int> vector2(SIZE, 2); 
        //serial computation 
        clock_t start_serial = clock(); 
        long long dot_product_serial = 0; 
        for (int i = 0; i < SIZE; ++i) { 
            dot_product_serial += vector1[i] * vector2[i]; 
        } 
        clock_t end_serial = clock(); 
        double serial_time = static_cast<double>(end_serial - start_serial) /  
        CLOCKS_PER_SEC; 
        std::cout << "Serial Dot Product: " << dot_product_serial << "\n"; 
        std::cout << "Serial Time: " << serial_time << " seconds\n"; 
        //parallel computation 
        clock_t start_parallel = clock(); 
 long long dot_product_parallel = 0; 
        #pragma omp parallel for reduction(+:dot_product_parallel) 
        for (int i = 0; i < SIZE; ++i) { 
            dot_product_parallel += vector1[i] * vector2[i]; 
        } 
        clock_t end_parallel = clock(); 
        double parallel_time = static_cast<double>(end_parallel - start_parallel) /  
        CLOCKS_PER_SEC; 
        std::cout << "Parallel Dot Product: " << dot_product_parallel << "\n"; 
        std::cout << "Parallel Time: " << parallel_time << " seconds"; 
        return 0; 
}

#------------------ Task2 -----------------------------
#include <iostream> 
#include <immintrin.h> 
// Function to perform SIMD vector addition 
void simdVectorAdd(float* a, float* b, float* result, int size) { 
 // Determine the number of floats that can be processed in a single SIMD  operation 
     const int simdSize = 8; // AVX supports 8 floats (32 bytes) 
     // Calculate the number of SIMD operations needed
      
     // Perform any remaining scalar operations 
     for (int i = simdIterations * simdSize; i < size; ++i) { 
        result[i] = a[i] + b[i]; 
     } 
} 
int main() { 
     const int size = 16; // Array size 
     // Initialize input arrays 
     float a[size] = { 1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f,  
    11.0f, 12.0f, 13.0f, 14.0f, 15.0f, 16.0f }; 
     float b[size] = { 16.0f, 15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f,  
    7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f }; 
     float result[size]; 
     // Perform SIMD vector addition 
     simdVectorAdd(a, b, result, size); 
     // Display the result 
     std::cout << "Vector Addition Result:" << std::endl; 
     for (int i = 0; i < size; ++i) { 
     std::cout << result[i] << " "; 
     } 

std::cout << std::endl; 
return 0; 
} 


#------------------ Task3 -----------------------------
#include <ctime> 
#include <vector> 
#include <omp.h> 
const int SIZE = 1000000; 
int main() { 
//initialize vectors 
std::vector <int> vector1(SIZE, 1); 
std::vector <int> vector2(SIZE, 2); 
//serial computation 
clock_t start_serial = clock(); 
long long sum_serial = 0; 
for (int i = 0; i < SIZE; ++i) { 
sum_serial += vector1[i] + vector2[i]; 
}
clock_t end_serial = clock(); 
double serial_time = static_cast<double>(end_serial - start_serial) /  
CLOCKS_PER_SEC;#include <iostream> 
std::cout << "Serial Sum: " << sum_serial << "\n"; 
std::cout << "Serial Time: " << serial_time << " seconds\n"; 
//parallel computation 
clock_t start_parallel = clock(); 
long long sum_parallel = 0; 
#pragma omp parallel for reduction(+:sum_parallel) 
for (int i = 0; i < SIZE; ++i) { 
sum_parallel += vector1[i] * vector2[i]; 
} 
clock_t end_parallel = clock(); 
double parallel_time = static_cast<double>(end_parallel - start_parallel) /  
CLOCKS_PER_SEC; 
std::cout << "Parallel Sum: " << dot_product_parallel << "\n"; 
std::cout << "Parallel Time: " << parallel_time << " seconds"; 
return 0; 
}


#------------------ Task4 -----------------------------
#include <vector> 
#include <omp.h> 
#include <iostream> 
const int SIZE = 100; 
int main() { 
//initialize vectors 
std::vector <int> vector1(SIZE, 1); 
std::vector <int> scaled(SIZE); 
//long long scaled = 0; 
#pragma omp parallel 
for (int i = 0; i < SIZE; ++i) { 
scaled[i] = vector1[i] * 2; 
std::cout << scaled[i] << " "; 
} 
std::cout << std::endl; 
return 0 
} 

#-----------------Task5 -----
# -----------------------
#include <ctime> 
#include <vector> 
#include <omp.h> 
#include <iostream> 
const int SIZE = 100; 
int main() { 
//initialize vectors 
std::vector <int> vector1(SIZE, 2); 
std::vector <int> squared(SIZE); 
#pragma omp parallel 
for (int i = 0; i < SIZE; ++i) { 
squared[i] = vector1[i] * vector1[1]; 
std::cout << squared[i] << " "; 
} 
std::cout << std::endl; 
}


#-----------------Lab9 ----------------------------
#-----------------Task1 ----------------------------



Import socket

import threading

def handle_client(client_socket):

request = client_socket.recv(1024)

print(f"Recieved: {{request}}")


client_socket.send(b"ACK!")

client_socket.close()


def run_server():


server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

server.bind(("0.0.0.0", 12345))

server.listen(5)


print("[*] Listening on 0.0.0.0:12345")


while True:


client, addr server.accept()


print("[*] Accepted connection from: {}:{}".format(addr[0], addr[1])) client_handler threading.Thread(target-handle_client, args=(client,))

client_handler.start()


if _name_ "_main_":


run_server()


#-----------------Task2 ----------------------------


import socket

def run_client():

client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)


client.connect(("127.0.0.1", 12345)) client.send(b"Hello world!")

response = client.recv(1024)

print(f"Server response: {response.decode('utf-8')}")

client.close()


if_name__ == "main_":

run_client()



#-----------------Task3 ----------------------------

handle_client(client_socket):

request client_socket.recv(1024) str_request request.decode("utf8")

print("Received: (str_request)")

if str_request = "quit":

send_strstr(time.time())+" Disconnected"

client_socket.send(bytes(send_str, 'utf-8'))

client_socket.close()

client_socket.send(bytes("Acknowledged", 'utf-8'))

client_socket.close()

def run_server():

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

server.bind(("0.0.0.0", 12345))

server.listen(5)

print("[*] Listening on 0.0.0.0:12345")


return



try:


run server()

while True:

client, addr = server.accept()

print("[*] Accepted connection from: {}:{}".format(addr[0], addr[1]))

client_handler threading.Thread(target-handle_client, args=(client,))

client_handler.start()

except KeyboardInterrupt:

print("Server shutting down...")

server.close()

if_name "main":

#-----------------lab11 ----------------------------
#-----------------Task1 ----------------------------

from pyspark.sql import SparkSession  
from pyspark.ml.feature import VectorAssembler  
from pyspark.ml.regression import LinearRegression  
# Create a Spark session  
spark = SparkSession.builder.appName("DistributedML").getOrCreate()  
# Sample data  
data = [(1.0, 2.0, 3.0), (2.0, 3.0, 4.0), (3.0, 4.0, 5.0)]  
columns = ["label", "feature1", "feature2"]  
# Create a DataFrame  
df = spark.createDataFrame(data, columns)  
# Assemble features into a vector  
assembler = VectorAssembler(inputCols=["feature1", "feature2"], 
outputCol="features")  
df = assembler.transform(df)  
# Define and train a linear regression model  
lr = LinearRegression(featuresCol="features", labelCol="label", 
predictionCol="prediction")  
model = lr.fit(df)  
# Make predictions  
predictions = model.transform(df)  
predictions.show()  


#-----------------Task2 ----------------------------
import tensorflow as tf from tensorflow.keras.models  
import Sequential from tensorflow.keras.layers  
import Dense # Create a TensorFlow distributed strategy  
strategy = tf.distribute.MirroredStrategy()  
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  
# Create and compile a simple model  
with strategy.scope(): model = Sequential([  
Dense(10, input_shape=(2,), activation='relu'),  
Dense(1, activation='linear')  
])  
model.compile(optimizer='adam', loss='mean_squared_error')  
# Sample data  
x_train = tf.constant([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])  
y_train = tf.constant([[3.0], [4.0], [5.0]])  
# Train the model  
model.fit(x_train, y_train, epochs=10)  
# Make predictions  
predictions = model.predict(x_train)  
print(predictions)  

#-----------------Task3 ----------------------------
import time  
# Record the start time  
start_time = time.time()  
from pyspark.sql import SparkSession  
from pyspark.ml.feature import VectorAssembler, StringIndexer  
from pyspark.ml.classification import RandomForestClassifier  
from pyspark.ml.evaluation import MulticlassClassificationEvaluator  
# Create a Spark session  
spark = SparkSession.builder.appName("DistributedML").getOrCreate()  
# Load the Iris dataset 
iris_data = spark.read.csv("/content/Iris.csv", header=True, inferSchema=True)  
# Feature engineering: Combine features into a single vector  
feature_columns = ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm",  
"PetalWidthCm"]  
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")  
iris_data = assembler.transform(iris_data)  
# Index the target column (species)  
indexer = StringIndexer(inputCol="Species", outputCol="label")  
iris_data = indexer.fit(iris_data).transform(iris_data)  
# Split the dataset into training and testing sets  
(training_data, testing_data) = iris_data.randomSplit([0.8, 0.2], seed=123)  
# Train a RandomForestClassifier model  
rf_classifier = RandomForestClassifier(featuresCol="features", 
labelCol="label", numTrees=10)  
model = rf_classifier.fit(training_data)  
# Make predictions on the testing set  
predictions = model.transform(testing_data)  
# Evaluate the model  
evaluator = MulticlassClassificationEvaluator(labelCol="label", 
predictionCol="prediction", metricName="accuracy")  
accuracy = evaluator.evaluate(predictions)  
print(f"Accuracy: {accuracy}")  
# Stop the Spark session  
spark.stop()  
# Record the end time  
end_time = time.time()  
# Calculate and print the execution time  
execution_time = end_time - start_time  
print(f"Execution Time: {execution_time} seconds") 

#-----------------Task4 ----------------------------
import pandas as pd import time  
# Record the start time  
start_time = time.time()  
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier  
from sklearn.metrics import accuracy_score  
# Load the Iris dataset  
iris_data = pd.read_csv("/content/Iris.csv")  
# Feature engineering: Combine features into a single array  
X = iris_data[["SepalLengthCm", "SepalWidthCm", "PetalLengthCm",  
"PetalWidthCm"]]  
y = iris_data["Species"]  
# Index the target column (species)  
from sklearn.preprocessing import LabelEncoder  
label_encoder = LabelEncoder()  
y = label_encoder.fit_transform(y)  
# Split the dataset into training and testing sets  
X_train, X_test, y_train, y_test = train_test_split(X, y,  
test_size=0.2, random_state=123)  
# Train a RandomForestClassifier model  
rf_classifier = RandomForestClassifier(n_estimators=10, random_state=123)  
rf_classifier.fit(X_train, y_train)  
# Make predictions on the testing set  
predictions = rf_classifier.predict(X_test)  
# Evaluate the model  
accuracy = accuracy_score(y_test, predictions)  
print(f"Accuracy: {accuracy}")  
# Record the end time  
end_time = time.time()  
# Calculate and print the execution time  
execution_time = end_time - start_time  
print(f"Execution Time: {execution_time} seconds")

#-----------------Task 5----------------------------
import time  
# Record the start time  
start_time = time.time()  
from pyspark.sql import SparkSession  
from pyspark.ml.feature import VectorAssembler, StringIndexer  
from pyspark.ml.classification import RandomForestClassifier  
from pyspark.ml.evaluation import MulticlassClassificationEvaluator  
# Create a Spark session  
spark = SparkSession.builder.appName("DistributedML").getOrCreate()
# Load the cummulative loss value dataset  
cummulative_data = spark.read.csv("/content/cummulative_loss_value.csv", 
header=True, inferSchema=True)  
# Feature engineering: Combine features into a single vector  
feature_columns = ["aircraft", "helicopter", "tank", "APC", "field artillery", 
"MRL", "drone", "naval ship", "anti-aircraft warfare"]  
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")  
cummulative_data = assembler.transform(cummulative_data)  
# Index the target column (military auto)  
indexer = StringIndexer(inputCol="military auto", outputCol="label")  
cummulative_data = indexer.fit(cummulative_data).transform(cummulative_data)  
# Split the dataset into training and testing sets  
(training_data, testing_data) = cummulative_data.randomSplit([0.8, 0.2], 
seed=123)  
# Train a RandomForestClassifier model  
rf_classifier = RandomForestClassifier(featuresCol="features", 
labelCol="label", numTrees=10)  
model = rf_classifier.fit(training_data)  
# Make predictions on the testing set  
predictions = model.transform(testing_data)  
# Evaluate the model  
evaluator = MulticlassClassificationEvaluator(labelCol="label", 
predictionCol="prediction", metricName="accuracy")  
accuracy = evaluator.evaluate(predictions)  
print(f"Accuracy: {accuracy}")  
# Stop the Spark session  
spark.stop()  
# Record the end time  
end_time = time.time()  
# Calculate and print the execution time  
execution_time = end_time - start_time  
print(f"Execution Time: {execution_time} seconds")  


#-----------------Task 6----------------------------

import pandas as pd import time  
# Record the start time  
start_time = time.time()  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.metrics import accuracy_score  
# Load the cummulative loss value dataset  
cummulative_data = pd.read_csv("/content/cummulative_loss_value.csv")  
# Feature engineering: Combine features into a single array  
X = cummulative_data[["aircraft", "helicopter", "tank", "APC", "field 
artillery", "MRL", "drone", "naval ship", "anti-aircraft warfare"]] y = 
cummulative_data["military auto"]  
# Index the target column (species)  
from sklearn.preprocessing import LabelEncoder  
label_encoder = LabelEncoder()  
y = label_encoder.fit_transform(y)  
# Split the dataset into training and testing sets  
X_train, X_test, y_train, y_test = train_test_split(X, y,  
test_size=0.2, random_state=123)  
# Train a RandomForestClassifier model  
rf_classifier = RandomForestClassifier(n_estimators=10, random_state=123)  
rf_classifier.fit(X_train, y_train)  
# Make predictions on the testing set  
predictions = rf_classifier.predict(X_test)  
# Evaluate the model  
accuracy = accuracy_score(y_test, predictions)  
print(f"Accuracy: {accuracy}")  
# Record the end time  
end_time = time.time()  
# Calculate and print the execution time  
execution_time = end_time - start_time  
print(f"Execution Time: {execution_time} seconds")  

